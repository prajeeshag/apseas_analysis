INFO:__main__:<xarray.Dataset> Size: 90GB
Dimensions:        (forecast: 48, member: 25, step: 6, isobaricInhPa: 20,
                    y: 390, x: 400)
Coordinates:
  * forecast       (forecast) datetime64[ns] 384B 2009-01-01 ... 2012-12-01
  * isobaricInhPa  (isobaricInhPa) float64 160B 1e+03 950.0 900.0 ... 100.0 50.0
    XLAT           (y, x) float64 1MB 9.56 9.565 9.571 ... 36.84 36.83 36.82
    XLONG          (y, x) float64 1MB 32.55 32.62 32.69 ... 64.5 64.58 64.67
Dimensions without coordinates: member, step, y, x
Data variables:
    hus            (forecast, member, step, isobaricInhPa, y, x) float32 90GB dask.array<chunksize=(1, 1, 1, 20, 390, 400), meta=np.ndarray>
Attributes:
    GRIB_edition:            2
    GRIB_centre:             kwbc
    GRIB_centreDescription:  US National Weather Service - NCEP
    GRIB_subCentre:          0
    Conventions:             CF-1.7
    institution:             US National Weather Service - NCEP
    history:                 2024-10-31T16:36 GRIB to CDM+CF via cfgrib-0.9.1...
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(0, 1, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(1, 2, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(2, 3, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(3, 4, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(4, 5, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(5, 6, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(6, 7, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(7, 8, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(8, 9, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(9, 10, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(10, 11, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(11, 12, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(12, 13, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(14, 15, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(13, 14, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(15, 16, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(16, 17, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(17, 18, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(18, 19, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(19, 20, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(20, 21, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(21, 22, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(22, 23, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(23, 24, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(24, 25, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(25, 26, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(26, 27, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(32, 33, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(41, 42, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(43, 44, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(45, 46, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(38, 39, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(28, 29, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(2, 3, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(44, 45, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(30, 31, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(36, 37, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(31, 32, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(27, 28, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(1, 2, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(39, 40, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(0, 1, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(37, 38, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(47, 48, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(35, 36, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(46, 47, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(42, 43, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(34, 35, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(33, 34, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(40, 41, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(29, 30, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(20, 21, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(4, 5, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(5, 6, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(6, 7, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(3, 4, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(9, 10, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(13, 14, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(23, 24, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(12, 13, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(25, 26, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(11, 12, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(14, 15, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(7, 8, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(10, 11, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(8, 9, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(29, 30, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(15, 16, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(16, 17, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(17, 18, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(24, 25, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(31, 32, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(18, 19, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(32, 33, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(19, 20, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(21, 22, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(22, 23, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(28, 29, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(27, 28, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(26, 27, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(30, 31, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(36, 37, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(33, 34, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(37, 38, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(38, 39, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(34, 35, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(35, 36, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:38:20,238 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.86 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:20,654 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.09 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:29,557 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.78 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:29,698 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 215.88 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:30,201 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 12.35 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:30,240 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.80 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:30,544 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.86 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:30,837 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.32 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:31,014 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.66 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:31,941 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.23 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:31,969 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.10 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:32,450 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.71 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:33,165 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.06 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:33,569 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.82 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:34,253 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.27 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:34,847 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.79 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:35,570 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.30 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:39,592 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 228.12 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:39,608 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 221.69 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:39,725 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.19 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:39,854 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 243.81 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:40,089 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 218.44 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:41,273 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 231.58 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:41,449 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.56 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:41,900 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.56 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:42,397 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:42,577 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 244.24 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:42,670 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:42,670 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:43,070 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35919 (pid=2531643) exceeded 95% memory budget. Restarting...
2024-10-31 16:38:43,460 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35919
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:48336 remote=tcp://127.0.0.1:35919>: Stream is closed
2024-10-31 16:38:43,426 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:36927 -> tcp://127.0.0.1:35919
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:36927 remote=tcp://127.0.0.1:57032>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:43,610 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 223.17 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:44,757 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 12.37 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:49,211 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:35919' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-69291e5de701d044e408da595094251f', ('open_dataset-q-62b31f329ec52a969b4eba0bdee2aebf', 0, 0, 0, 0), 'original-open_dataset-q-2f197742ec035bd68114d37867406ddc'} (stimulus_id='handle-worker-cleanup-1730381929.2106311')
2024-10-31 16:38:49,278 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35397 (pid=2531626) exceeded 95% memory budget. Restarting...
2024-10-31 16:38:49,543 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35919
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 546, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1472, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1493, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-10-31 16:38:49,513 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35919
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 559, in connect
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7fe1ec3f7ad0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/tasks.py", line 665, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1472, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1493, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-10-31 16:38:49,661 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:38:49,690 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:35397' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('open_dataset-q-e71308d12b3d690fb7bad6825474e686', 0, 0, 0, 0), ('_access_through_series-1ea8633501af7add0e2d91fb7bb9888b', 0)} (stimulus_id='handle-worker-cleanup-1730381929.6886415')
2024-10-31 16:38:49,688 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35397
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50130 remote=tcp://127.0.0.1:35397>: Stream is closed
2024-10-31 16:38:49,688 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35397
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50136 remote=tcp://127.0.0.1:35397>: Stream is closed
2024-10-31 16:38:49,688 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35397
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50114 remote=tcp://127.0.0.1:35397>: Stream is closed
2024-10-31 16:38:49,688 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35397
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50102 remote=tcp://127.0.0.1:35397>: Stream is closed
2024-10-31 16:38:49,688 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35397
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50110 remote=tcp://127.0.0.1:35397>: Stream is closed
2024-10-31 16:38:49,800 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:49,846 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:49,948 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.55 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:50,094 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.07 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:50,190 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.56 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:50,254 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.55 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:50,266 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.59 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:50,318 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:50,371 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.25 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:50,545 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:50,726 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 11.03 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:51,270 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.55 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:54,881 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:54,928 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46595 (pid=2531581) exceeded 95% memory budget. Restarting...
2024-10-31 16:38:54,935 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36783 (pid=2531601) exceeded 95% memory budget. Restarting...
2024-10-31 16:38:54,937 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35671 (pid=2531611) exceeded 95% memory budget. Restarting...
2024-10-31 16:38:54,965 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:38:55,134 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46595
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59786 remote=tcp://127.0.0.1:46595>: Stream is closed
2024-10-31 16:38:55,134 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46595
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59068 remote=tcp://127.0.0.1:46595>: Stream is closed
2024-10-31 16:38:55,135 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46595
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59040 remote=tcp://127.0.0.1:46595>: Stream is closed
2024-10-31 16:38:55,134 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46595
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59082 remote=tcp://127.0.0.1:46595>: Stream is closed
2024-10-31 16:38:55,147 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:46595' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('open_dataset-q-9b1d37752e062efb2fc9fb9c974ab86c', 0, 0, 0, 0), 'original-open_dataset-q-e3eb2cf3af5977b39309f910e6685308', 'original-open_dataset-q-e8657110de582e542b3a3f55a52015ff'} (stimulus_id='handle-worker-cleanup-1730381935.1468143')
2024-10-31 16:38:55,134 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46595
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59054 remote=tcp://127.0.0.1:46595>: Stream is closed
2024-10-31 16:38:55,134 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46595
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59096 remote=tcp://127.0.0.1:46595>: Stream is closed
2024-10-31 16:38:55,663 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36783
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47804 remote=tcp://127.0.0.1:36783>: Stream is closed
2024-10-31 16:38:55,663 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36783
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:32868 remote=tcp://127.0.0.1:36783>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,663 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36783
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:32880 remote=tcp://127.0.0.1:36783>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,663 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36783
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:32886 remote=tcp://127.0.0.1:36783>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,663 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36783
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:32872 remote=tcp://127.0.0.1:36783>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,768 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35671
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44764 remote=tcp://127.0.0.1:35671>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,768 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35671
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44752 remote=tcp://127.0.0.1:35671>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,839 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.20 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:55,843 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35671
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44744 remote=tcp://127.0.0.1:35671>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,840 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:39845 -> tcp://127.0.0.1:35671
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:39845 remote=tcp://127.0.0.1:40092>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,848 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35671
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44772 remote=tcp://127.0.0.1:35671>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:38:55,852 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.07 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:56,019 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 12.00 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:56,320 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.15 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:56,468 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.09 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:56,744 - distributed.worker.memory - WARNING - Worker is at 2% memory usage. Resuming worker. Process memory: 364.22 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:57,126 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.06 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:57,547 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 12.56 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:58,111 - distributed.worker.memory - WARNING - Worker is at 13% memory usage. Resuming worker. Process memory: 2.03 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:59,284 - distributed.worker.memory - WARNING - Worker is at 17% memory usage. Resuming worker. Process memory: 2.67 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:59,506 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.04 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:38:59,578 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39845 (pid=2531574) exceeded 95% memory budget. Restarting...
2024-10-31 16:38:59,616 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:35671' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-e93cd4c273d3ecc9b807322441d4b791', 'original-open_dataset-q-d718e5dbf85792a1a2bcac824cb89172', ('open_dataset-q-9af35c028e9993b0bc290d92bfeeae3c', 0, 0, 0, 0), 'original-open_dataset-q-04d5cecc1bfae8c167291ae1232ffd6d'} (stimulus_id='handle-worker-cleanup-1730381939.6150172')
2024-10-31 16:38:59,625 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:36783' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('open_dataset-q-677f1796b97c0034189ae72cd7a85ec0', 0, 0, 0, 0), 'original-open_dataset-q-3d199855fa5c714faf32f0c0361e632a'} (stimulus_id='handle-worker-cleanup-1730381939.6247518')
2024-10-31 16:38:59,657 - distributed.diskutils - ERROR - Failed to remove '/tmp/dask-scratch-space/worker-e11gzkbn/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'
2024-10-31 16:38:59,662 - distributed.diskutils - ERROR - Failed to remove '/tmp/dask-scratch-space/worker-e11gzkbn' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/tmp/dask-scratch-space/worker-e11gzkbn'
/scratch/athippp/iops/micromamba/lib/python3.12/contextlib.py:144: UserWarning: Creating scratch directories is taking a surprisingly long time. (2.25s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
2024-10-31 16:38:59,758 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35671
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44750 remote=tcp://127.0.0.1:35671>: Stream is closed
2024-10-31 16:38:59,967 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.05 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:00,112 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:00,220 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:00,233 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:00,233 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35397
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50144 remote=tcp://127.0.0.1:35397>: Stream is closed
2024-10-31 16:39:00,311 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:43215 -> tcp://127.0.0.1:36783
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1777, in get_data
    compressed = await comm.write(msg, serializers=serializers)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:43215 remote=tcp://127.0.0.1:59238>: Stream is closed
2024-10-31 16:39:01,302 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.12 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:01,913 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 10.61 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:02,215 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 12.36 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:02,407 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:39845' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-valid_time-2187ea7b9120b30dad5b0765a813aba1', ('open_dataset-q-3b6dc5a74efd76a8e462104ffce242c3', 0, 0, 0, 0), 'original-open_dataset-q-30b74008ba4d9d8522fe47371ce481d6'} (stimulus_id='handle-worker-cleanup-1730381942.4078572')
2024-10-31 16:39:02,467 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(29, 30, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:02,676 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.06 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:02,814 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 10.61 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:03,038 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43539 (pid=2531586) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:03,114 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 12.35 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:03,252 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 12.61 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:03,347 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43539
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:36946 remote=tcp://127.0.0.1:43539>: Stream is closed
2024-10-31 16:39:03,348 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:43215 -> tcp://127.0.0.1:43539
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:43215 remote=tcp://127.0.0.1:42386>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:03,348 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43539
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37236 remote=tcp://127.0.0.1:43539>: Stream is closed
2024-10-31 16:39:03,348 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43539
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37260 remote=tcp://127.0.0.1:43539>: Stream is closed
2024-10-31 16:39:03,349 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43539
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37252 remote=tcp://127.0.0.1:43539>: Stream is closed
2024-10-31 16:39:03,348 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43539
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37264 remote=tcp://127.0.0.1:43539>: Stream is closed
2024-10-31 16:39:03,391 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:43539' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-9af35c028e9993b0bc290d92bfeeae3c'} (stimulus_id='handle-worker-cleanup-1730381943.391568')
2024-10-31 16:39:03,397 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:03,412 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.21 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:03,442 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43215 (pid=2531622) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:04,038 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45227 (pid=2531579) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:04,120 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:45227' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-9b1d37752e062efb2fc9fb9c974ab86c'} (stimulus_id='handle-worker-cleanup-1730381944.1193724')
2024-10-31 16:39:04,147 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:04,175 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:43215' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('open_dataset-q-4666f44d2927a27b0a2eb6821f208486', 0, 0, 0, 0), 'original-open_dataset-q-30b74008ba4d9d8522fe47371ce481d6'} (stimulus_id='handle-worker-cleanup-1730381944.1750536')
2024-10-31 16:39:04,168 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43215
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59190 remote=tcp://127.0.0.1:43215>: Stream is closed
2024-10-31 16:39:05,212 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 12.40 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:05,226 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43539
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37248 remote=tcp://127.0.0.1:43539>: Stream is closed
2024-10-31 16:39:05,229 - distributed.worker.memory - WARNING - Worker is at 15% memory usage. Resuming worker. Process memory: 2.32 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:05,463 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43215
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:42370 remote=tcp://127.0.0.1:43215>: Stream is closed
2024-10-31 16:39:06,097 - distributed.worker.memory - WARNING - Worker is at 6% memory usage. Resuming worker. Process memory: 0.97 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:06,152 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43215
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 559, in connect
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7f42d4f3c710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/tasks.py", line 665, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1472, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1493, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-10-31 16:39:06,152 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43215
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 559, in connect
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7f65a938a450>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/tasks.py", line 665, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1472, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1493, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-10-31 16:39:06,258 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(40, 41, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(42, 43, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(39, 40, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:06,666 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.19 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:06,766 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.27 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:07,572 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 12.44 GiB -- Worker memory limit: 15.05 GiB
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(45, 46, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:07,836 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.11 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:08,073 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.05 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:08,517 - distributed.worker.memory - WARNING - Worker is at 7% memory usage. Resuming worker. Process memory: 1.07 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:08,530 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 12.40 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:08,756 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:32913 (pid=2531647) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:08,821 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 10.80 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:08,836 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:32913' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-2b165f7480c8827c46dd472e06a9c87e', 'original-open_dataset-q-7a384f78146c60f15b6f86d0f9efc708'} (stimulus_id='handle-worker-cleanup-1730381948.83686')
2024-10-31 16:39:08,847 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(1, 2, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:08,966 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.26 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:09,207 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.09 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:09,919 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 10.77 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:10,679 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33305 (pid=2531635) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:10,817 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:33305' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-bb6423251350c72529fa0465caf15f6c', 'original-open_dataset-valid_time-e95e8d0954ded1d47ab07981d144c238'} (stimulus_id='handle-worker-cleanup-1730381950.8169954')
2024-10-31 16:39:10,850 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:11,261 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.73 GiB -- Worker memory limit: 15.05 GiB
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(5, 6, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:11,899 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.15 GiB -- Worker memory limit: 15.05 GiB
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(6, 7, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:12,383 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.15 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:13,822 - distributed.worker.memory - WARNING - Worker is at 17% memory usage. Resuming worker. Process memory: 2.66 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:14,538 - distributed.worker.memory - WARNING - Worker is at 7% memory usage. Resuming worker. Process memory: 1.17 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:14,644 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46827 (pid=2531638) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:14,708 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46827
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53736 remote=tcp://127.0.0.1:46827>: Stream is closed
2024-10-31 16:39:14,709 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46827
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:53740 remote=tcp://127.0.0.1:46827>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:14,719 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:46827' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-0e22657cd598ec82f631377c16971b94', ('open_dataset-q-c34acf18edfdafb10c83707c3d9c444e', 0, 0, 0, 0)} (stimulus_id='handle-worker-cleanup-1730381954.719708')
2024-10-31 16:39:14,708 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46827
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:53756 remote=tcp://127.0.0.1:46827>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:14,775 - distributed.worker.memory - WARNING - Worker is at 4% memory usage. Resuming worker. Process memory: 623.46 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:15,809 - distributed.worker.memory - WARNING - Worker is at 4% memory usage. Resuming worker. Process memory: 634.92 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:15,921 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.29 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:16,314 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.80 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:16,636 - distributed.worker.memory - WARNING - Worker is at 10% memory usage. Resuming worker. Process memory: 1.64 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:16,790 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.13 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:17,737 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43541 (pid=2531651) exceeded 95% memory budget. Restarting...
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(8, 9, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(9, 10, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:17,832 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:17,840 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43541 (pid=2531651) is slow to accept SIGTERM; sending SIGKILL
2024-10-31 16:39:17,853 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43541 (pid=2531651) is slow to accept SIGTERM; sending SIGKILL
2024-10-31 16:39:17,944 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43541 (pid=2531651) is slow to accept SIGTERM; sending SIGKILL
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(2, 3, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:18,052 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43541 (pid=2531651) is slow to accept SIGTERM; sending SIGKILL
2024-10-31 16:39:18,144 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43541 (pid=2531651) is slow to accept SIGTERM; sending SIGKILL
2024-10-31 16:39:18,259 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43541 (pid=2531651) is slow to accept SIGTERM; sending SIGKILL
2024-10-31 16:39:18,299 - distributed.worker.memory - WARNING - Worker is at 5% memory usage. Resuming worker. Process memory: 822.35 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:18,427 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43541
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47158 remote=tcp://127.0.0.1:43541>: Stream is closed
2024-10-31 16:39:18,427 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43541
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47168 remote=tcp://127.0.0.1:43541>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:18,427 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43541
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47176 remote=tcp://127.0.0.1:43541>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:18,427 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43541
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47186 remote=tcp://127.0.0.1:43541>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:18,438 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:43541' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('open_dataset-q-657d824b851482ae04a18877f04905c0', 0, 0, 0, 0), 'original-open_dataset-q-f0120f898de783191e89e67ddb0c50f2', ('store-map-6364af9965d0b553803e41020c6c97e5', 0, 0, 2, 0, 0, 0)} (stimulus_id='handle-worker-cleanup-1730381958.4382272')
2024-10-31 16:39:19,424 - distributed.worker.memory - WARNING - Worker is at 1% memory usage. Resuming worker. Process memory: 274.12 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:20,387 - distributed.worker.memory - WARNING - Worker is at 9% memory usage. Resuming worker. Process memory: 1.37 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:22,364 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45653 (pid=2541050) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:22,434 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:32865 -> tcp://127.0.0.1:45653
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:32865 remote=tcp://127.0.0.1:37914>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:22,538 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:22,675 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:45653' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-valid_time-bb4841bcd75a08a311ab942bafccc119', 'original-open_dataset-q-04d5cecc1bfae8c167291ae1232ffd6d'} (stimulus_id='handle-worker-cleanup-1730381962.6754248')
2024-10-31 16:39:22,724 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(11, 12, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:23,143 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36927 (pid=2531606) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:23,224 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36927
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:39272 remote=tcp://127.0.0.1:36927>: Stream is closed
2024-10-31 16:39:26,013 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:36927' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-valid_time-d8be2ffbe07f92c3b50768a4371a15ba', 'original-open_dataset-q-cf2052f8e85eed59089305ace77e85ec', 'original-open_dataset-valid_time-fdfcad387301898f08876875c60f0a6f', ('broadcast_to-concatenate-getitem-store-map-406071c14ad7045c204bc9a1f4cd585c', 0, 0, 5, 0, 0, 0), ('open_dataset-q-d79467957f309ee0b3b1d432808261a1', 0, 0, 0, 0)} (stimulus_id='handle-worker-cleanup-1730381966.0093257')
2024-10-31 16:39:26,130 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:29,859 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.24 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:29,899 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.11 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:30,180 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38527 (pid=2531630) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:30,271 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38527
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:40952 remote=tcp://127.0.0.1:38527>: Stream is closed
2024-10-31 16:39:30,271 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38527
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:40968 remote=tcp://127.0.0.1:38527>: Stream is closed
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(4, 5, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:31,917 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.62 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:32,070 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.62 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:32,165 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.05 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:32,542 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:38527' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-f0120f898de783191e89e67ddb0c50f2', ('open_dataset-q-3d4a342e079a33f9042c8b4a4753d951', 0, 0, 0, 0), 'original-open_dataset-q-e3eb2cf3af5977b39309f910e6685308'} (stimulus_id='handle-worker-cleanup-1730381972.541957')
2024-10-31 16:39:32,732 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38527
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 559, in connect
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7f65a8ee5970>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/tasks.py", line 665, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1472, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1493, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-10-31 16:39:32,749 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39955 (pid=2531663) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:32,776 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:32,735 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38527
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 559, in connect
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7f59bcc73e30>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1481, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1425, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/tasks.py", line 665, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1472, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/scratch/athippp/iops/micromamba/lib/python3.12/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2871, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1493, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-10-31 16:39:33,105 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39955
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:34498 remote=tcp://127.0.0.1:39955>: Stream is closed
2024-10-31 16:39:33,115 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:39955' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('open_dataset-q-f0628172f6050fdfc6fa926b2186d566', 0, 0, 0, 0), 'original-open_dataset-q-3b6dc5a74efd76a8e462104ffce242c3'} (stimulus_id='handle-worker-cleanup-1730381973.1151326')
2024-10-31 16:39:33,684 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.15 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:33,740 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.24 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:35,058 - distributed.worker.memory - WARNING - Worker is at 5% memory usage. Resuming worker. Process memory: 835.71 MiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:35,501 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42721 (pid=2531570) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:35,511 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43095 (pid=2541082) exceeded 95% memory budget. Restarting...
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(44, 45, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:35,615 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:33531 -> tcp://127.0.0.1:43095
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:33531 remote=tcp://127.0.0.1:59036>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:35,640 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:43095' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-d718e5dbf85792a1a2bcac824cb89172'} (stimulus_id='handle-worker-cleanup-1730381975.6401122')
2024-10-31 16:39:35,642 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:40535 -> tcp://127.0.0.1:42721
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:40535 remote=tcp://127.0.0.1:37606>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:35,650 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:42721' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-081bd0409446962e6eab2586c02459bc'} (stimulus_id='handle-worker-cleanup-1730381975.6499405')
2024-10-31 16:39:35,674 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(24, 25, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(13, 14, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:35,903 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.62 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:35,917 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(35, 36, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:36,030 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(12, 13, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:36,257 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33529 (pid=2540408) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:36,309 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:33531 -> tcp://127.0.0.1:33529
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:33531 remote=tcp://127.0.0.1:59046>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:36,315 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:36,763 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 11.69 GiB -- Worker memory limit: 15.05 GiB
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(4, 5, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:36,932 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 12.22 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:37,138 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38973 (pid=2541318) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:37,200 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:38973' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-cf2052f8e85eed59089305ace77e85ec', 'original-open_dataset-q-e3eb2cf3af5977b39309f910e6685308'} (stimulus_id='handle-worker-cleanup-1730381977.2004845')
2024-10-31 16:39:37,208 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:37,289 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:40535 -> tcp://127.0.0.1:38973
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1124, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:40535 remote=tcp://127.0.0.1:33178>: BrokenPipeError: [Errno 32] Broken pipe
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(28, 29, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(25, 26, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(35, 36, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(1, 2, None), 'forecast': slice(47, 48, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(7, 8, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:41,277 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.08 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:41,301 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.57 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:41,678 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.56 GiB -- Worker memory limit: 15.05 GiB
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(18, 19, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(0, 1, None), 'forecast': slice(30, 31, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(10, 11, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:45,741 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41027 (pid=2541288) exceeded 95% memory budget. Restarting...
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(14, 15, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:45,786 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:41027' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-e08e414ab69640756598f2139c46b7a7'} (stimulus_id='handle-worker-cleanup-1730381985.7866778')
2024-10-31 16:39:45,856 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:40535 -> tcp://127.0.0.1:41027
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1124, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:40535 remote=tcp://127.0.0.1:52292>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:46,122 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:46,280 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.09 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:39:46,574 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.16 GiB -- Worker memory limit: 15.05 GiB
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(21, 22, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:48,038 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:37843 (pid=2543423) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:48,090 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:37843' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-4666f44d2927a27b0a2eb6821f208486', 'original-open_dataset-q-cf2052f8e85eed59089305ace77e85ec'} (stimulus_id='handle-worker-cleanup-1730381988.0908103')
2024-10-31 16:39:48,091 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:40535 -> tcp://127.0.0.1:37843
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:40535 remote=tcp://127.0.0.1:52318>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:48,101 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:39:48,344 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41333 (pid=2543438) exceeded 95% memory budget. Restarting...
2024-10-31 16:39:48,385 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:41333' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-valid_time-d8be2ffbe07f92c3b50768a4371a15ba', 'original-open_dataset-q-960b1199afe6f88afce59f41e67d832f'} (stimulus_id='handle-worker-cleanup-1730381988.3854709')
2024-10-31 16:39:48,385 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:40535 -> tcp://127.0.0.1:41333
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 1778, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:40535 remote=tcp://127.0.0.1:52304>: ConnectionResetError: [Errno 104] Connection reset by peer
2024-10-31 16:39:48,391 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(40, 41, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(43, 44, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(42, 43, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(39, 40, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(3, 4, None), 'forecast': slice(0, 1, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(38, 39, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:39:59,976 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 12.54 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:40:00,045 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33531 (pid=2531658) exceeded 95% memory budget. Restarting...
2024-10-31 16:40:00,689 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:33531' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-6cf25703b2d9bff89b253f3842aff1f7'} (stimulus_id='handle-worker-cleanup-1730382000.6891942')
2024-10-31 16:40:00,695 - distributed.nanny - WARNING - Restarting worker
2024-10-31 16:40:01,680 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 12.62 GiB -- Worker memory limit: 15.05 GiB
2024-10-31 16:40:02,344 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40535 (pid=2531654) exceeded 95% memory budget. Restarting...
2024-10-31 16:40:02,417 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40535
Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 227, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 366, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker.py", line 2874, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/core.py", line 1015, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59144 remote=tcp://127.0.0.1:40535>: Stream is closed
2024-10-31 16:40:02,431 - distributed.scheduler - ERROR - Task ('broadcast_to-concatenate-getitem-store-map-de1674e7c017026cd70b721b0c651fa3', 0, 0, 5, 0, 0, 0) marked as failed because 4 workers died while trying to run it
2024-10-31 16:40:02,431 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:40535' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('open_dataset-q-cee1b4aee5b9045c30bdabca534abb80', 0, 0, 0, 0)} (stimulus_id='handle-worker-cleanup-1730382002.4310133')
INFO:__main__:Writing {'member': slice(3, 4, None), 'forecast': slice(3, 4, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:40:04,227 - distributed.nanny - WARNING - Restarting worker
INFO:__main__:Writing {'member': slice(3, 4, None), 'forecast': slice(6, 7, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(3, 4, None), 'forecast': slice(9, 10, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(44, 45, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(26, 27, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:40:05,337 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 10.53 GiB -- Worker memory limit: 15.05 GiB
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(41, 42, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(3, 4, None), 'forecast': slice(16, 17, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(3, 4, None), 'forecast': slice(17, 18, None)} to data/ap84SeasRF/hus.zarr
INFO:__main__:Writing {'member': slice(2, 3, None), 'forecast': slice(17, 18, None)} to data/ap84SeasRF/hus.zarr
2024-10-31 16:40:10,435 - distributed.worker - ERROR - Compute Failed
Key:       _write_to_zarr-dff87ee23fd75665e65563557be2e775
State:     long-running
Function:  _write_to_zarr
args:      ('/scratch/athippp/cylc-archive/ap84SeasRF/20120802T0000Z/mem1/outputs/wrf_isobaricInhPa_q.grb2', 'data/ap84SeasRF/hus.zarr', {'member': slice(0, 1, None), 'forecast': slice(43, 44, None)})
kwargs:    {}
Exception: "KilledWorker(('broadcast_to-concatenate-getitem-store-map-de1674e7c017026cd70b721b0c651fa3', 0, 0, 5, 0, 0, 0), <WorkerState 'tcp://127.0.0.1:40535', name: 21, status: closed, memory: 0, processing: 0>, 3)"
Traceback: '  File "/scratch/athippp/work/analysis/apseas5_grib_to_zarr.py", line 34, in _write_to_zarr\n    ds.to_zarr(store, region=region)\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/xarray/core/dataset.py", line 2548, in to_zarr\n    return to_zarr(  # type: ignore[call-overload,misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/xarray/backends/api.py", line 1698, in to_zarr\n    writes = writer.sync(\n             ^^^^^^^^^^^^\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/xarray/backends/common.py", line 297, in sync\n    delayed_store = chunkmanager.store(\n                    ^^^^^^^^^^^^^^^^^^^\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/xarray/namedarray/daskmanager.py", line 249, in store\n    return store(\n           ^^^^^^\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/dask/array/core.py", line 1230, in store\n    compute_as_if_collection(Array, store_dsk, map_keys, **kwargs)\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/dask/base.py", line 401, in compute_as_if_collection\n    return schedule(dsk2, keys, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/client.py", line 3480, in get\n    results = self.gather(packed, asynchronous=asynchronous, direct=direct)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/client.py", line 2553, in gather\n    return self.sync(\n           ^^^^^^^^^^\n  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/client.py", line 2414, in _gather\n    raise exception.with_traceback(traceback)\n'

2024-10-31 16:40:10,504 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-7b3a429d752119c26f4df9fb12d818ae')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,504 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-8567c1b1e1d35885f87990ebeed738ab')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,504 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-122e6a7bde88bdd655fd473fbba6aac4')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,504 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-10dc377af2aacc57abe0254e614e5df7')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,504 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-784b04f850e947de47a45c82cec78f77', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,504 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-c413eb62fbf175be8b490666f9ad1aa7')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-e08e414ab69640756598f2139c46b7a7', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-c89461e2e29096b8b4b7637f66fa105e')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-ce185e09e722989be4460640045ca33d', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-03f1cf608cee2bcb205da8d45625d057')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-bbe2d5003d257878b2a777b8f33c9c9d')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-c9d9600e85e5651661af735dc3966b53')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-abc4957c20156efa507fe38bdbeea222')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-c42a8192649e6f2fb165e946fe04550b', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,505 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-c11e680b25901e5ef7f3545dc232954e')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,506 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-f76fa7cb5064752948114a3e59c352fe')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,506 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-04d5cecc1bfae8c167291ae1232ffd6d', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-b89714e3e462c41679a9b69e438edfa9')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-12384f4a2201e4490ce630b3eb5732a5', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-8942d5534922f202fa8481d30137cc75')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-e86281e616c5989f515f18e29fcde8eb')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-081bd0409446962e6eab2586c02459bc', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-6f99b381f6b5e83f47b6799506d2869b')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-64deec1eb7c6b0595001ef57b69c05e5')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-0362b0659cf7a35abf17d1b93e40f51e')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-59bd3865dcfaf06e1c952f43c7bf2c54', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-3e65399575ee694782ecb87174af4b8a')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-9c73be34d8638df20315aca88e1c4688', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-f5d9518d0e65a54d21d3c6507bd985fd')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,507 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-dc74f36157c209072496dc03f4615546')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-5f72674f329c2ab496d6f6c8744a7fec')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-a2cf3b30a05f2dc55ac32852ec4da64e')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-667d8965acbec4a812127164881b2c14')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-ec5d99c400e98744e598d41d5c5e1503')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-90801bed1a484e996625a6c2e584907c')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-ca6bc81949f5109594907584faa1cd36')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-8c728a4f800d3b97ee73ba435d8a81c1')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-fff567d15509a35b4fa8f1c5cc1b8623')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-e3eb2cf3af5977b39309f910e6685308', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-55ac05aa9de9d5af44b7db28088f9fb9', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-0f42e4b6193c1a0d8241ba403cf87958')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,509 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-b3e152a1da0ca8d8539c00c67f2f3eba')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,509 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-357eb3bf7055074b79b3013785417402')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,509 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-2f323f2fe13a1cb2e47b9a8ff21df785')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-7b64f9bab537eebedbddad8fad914b9a')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-13e80386a262117ee3c26ef05d8be1d4')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-1e1f95db7857a286a16e160acdf370f9')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-bc538d1a171ce0b029f863c2c4398b75')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-c0eab31d4801ea736c84ae7f8b49151f')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-3a01e93feb4e330baa1ecf86bdd228d4')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-657d824b851482ae04a18877f04905c0', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-daf14907d8709e24866236fc07bc27fd')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,510 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-30ad2a76ee544137ca5534c034e969ff')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,511 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-d4bc2c9d81e57a0f820860a6484a4075')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,511 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-ca0db7e076387fae7ce3dfeacdb4b684')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,511 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-79ff8abda1b7824d0a5c85662cb99437')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,511 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-f6c57260c6bc49e7c0f8590e25022f08')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,511 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-0d40e5461cc319769afd8ef6812b8677')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,511 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-bac90449457bda232580949eb19fbf85', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,512 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-04d30a670ab993617c6c3c553d958c79')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,512 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-7d716cce4deae2c8ac79b9605d85821c')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,512 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-d685a3499d338e92bcfdc9315a86edcb')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,512 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-ac14556aac4c3f7cd920db700c0d1e2c')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,513 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-cf2052f8e85eed59089305ace77e85ec', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,513 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-7d223b804457ee42abde60cfd79aa395')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,513 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-0fe8053d583083feadb7f3836d58d587')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,513 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-b492b0a0fa5dcdbedf60371552bce656')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,513 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-52c801a637a57c4884e95d41c58add3c')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,514 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-cc287a437217e60847cf90255e11bb12')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,514 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-feb5b1aae9e5762ba79b546092f322e3')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,514 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-36738e6e020c688ac248bd61044da963')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,514 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-742d0edb1b2a0eb272172068a4a70355')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,515 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('open_dataset-q-bb4340358fe46a634d61fdb4f73cbc29', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,515 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('_write_to_zarr-5e66ec2029a3531a0f7544993d12b5d1')" coro=<Worker.execute() done, defined at /scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/worker_state_machine.py:3609>> ended with CancelledError
2024-10-31 16:40:10,564 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:41795' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-q-0e22657cd598ec82f631377c16971b94'} (stimulus_id='handle-worker-cleanup-1730382010.5645816')
2024-10-31 16:40:14,503 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,503 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,504 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,506 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,506 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,507 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,507 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,508 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,508 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,508 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,509 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,509 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,509 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,510 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,510 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,510 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,510 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,511 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,511 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,512 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,512 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,512 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,512 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
2024-10-31 16:40:14,513 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing
Traceback (most recent call last):
  File "/scratch/athippp/work/analysis/apseas5_grib_to_zarr.py", line 101, in <module>
    client.gather(futures)
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/client.py", line 2553, in gather
    return self.sync(
           ^^^^^^^^^^
  File "/scratch/athippp/work/analysis/apseas5_grib_to_zarr.py", line 34, in _write_to_zarr
    ds.to_zarr(store, region=region)
      ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/xarray/core/dataset.py", line 2548, in to_zarr
    return to_zarr(  # type: ignore[call-overload,misc]
  ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/xarray/backends/api.py", line 1698, in to_zarr
    writes = writer.sync(
      ^^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/xarray/backends/common.py", line 297, in sync
    delayed_store = chunkmanager.store(
^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/xarray/namedarray/daskmanager.py", line 249, in store
    return store(
  ^^^^^^^^^^^^^^^^
  File "/scratch/athippp/iops/micromamba/lib/python3.12/site-packages/distributed/client.py", line 2414, in _gather
    raise exception.with_traceback(traceback)
^^^
distributed.scheduler.KilledWorker: Attempted to run task ('broadcast_to-concatenate-getitem-store-map-de1674e7c017026cd70b721b0c651fa3', 0, 0, 5, 0, 0, 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:40535. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
/scratch/athippp/iops/micromamba/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 37 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
